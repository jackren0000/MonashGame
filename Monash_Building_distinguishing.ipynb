{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Mount to Google Drive**"
      ],
      "metadata": {
        "id": "yIHJp3bclRif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lCyGLaLtlRD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/MonashBuildings/"
      ],
      "metadata": {
        "id": "Wvl9VgYKnn5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialise the dataset**"
      ],
      "metadata": {
        "id": "GQ61vUsbQY-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "# ImageFolder treats images in different folder as different classes\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "ARU0sCPvQr_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the training data\n",
        "trainset = ImageFolder(root = './train', transform = transform)"
      ],
      "metadata": {
        "id": "XbFQXce0irPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Normalization**"
      ],
      "metadata": {
        "id": "LPqH8DVWEP1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "ni2GCXelEVp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size,\n",
        "                                          shuffle = True, num_workers = 2)"
      ],
      "metadata": {
        "id": "-r0s0m6SEyKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize the input**"
      ],
      "metadata": {
        "id": "ZLGIZM8hJD-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iOhWUig4JKIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "images,labels = next(dataiter)\n",
        "# This is batch[1]"
      ],
      "metadata": {
        "id": "YnLID8mop-sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "# There are 4 images in one batch"
      ],
      "metadata": {
        "id": "7HU61pQNp_Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "images,labels = next(dataiter)\n",
        "# This is batch[1]\n",
        "\n",
        "def imshow(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    img = img.numpy().transpose((1, 2, 0))  # Convert to numpy and reorder dimensions\n",
        "    print(img.shape)\n",
        "    img = std * img + mean  # Unnormalize\n",
        "    img = np.clip(img, 0, 1)  # Clip values to be between 0 and 1\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "imshow(images[0])\n",
        "print(f'The label for is {labels[0]}')\n",
        "imshow(images[2])\n",
        "print(f'The label for is {labels[2]}')"
      ],
      "metadata": {
        "id": "wCsI4LNjJW2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the CNN model**"
      ],
      "metadata": {
        "id": "3BMKvbCPBSo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmR1n8Nz-inR"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, num_classes = 3):\n",
        "    super().__init__()\n",
        "    # First convolutional layer\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool_1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # Second convolutional layer\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.pool_2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    self.fc1 = nn.Linear(53 * 53 * 16, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Pass the first convolutional layer\n",
        "    x = self.pool_1(F.relu(self.conv1(x)))\n",
        "\n",
        "    # Pass the second convolutional layer\n",
        "    x = self.pool_2(F.relu(self.conv2(x)))\n",
        "\n",
        "    # Flatten x into one dimension\n",
        "    x = torch.flatten(x, 1)\n",
        "\n",
        "    # Pass the fully connected layer\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def predict(self, x):\n",
        "    predictions = self(x)\n",
        "\n",
        "    # Get the predicted classes\n",
        "    _, predicted_classes = torch.max(predictions, dim=1)\n",
        "\n",
        "    return predicted_classes, x\n",
        "\n",
        "\n",
        "CNN_model =  CNN()"
      ],
      "metadata": {
        "id": "fqN71CTo_Fhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(CNN_model.parameters(), lr = 0.001, momentum =0.9)"
      ],
      "metadata": {
        "id": "OeYIQBoPOaeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Begin training**"
      ],
      "metadata": {
        "id": "ViacnF6UBXqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_loss_mat = []\n",
        "for epoch in range(15):\n",
        "  batch_loss = 0.0\n",
        "  epoch_loss = 0.0\n",
        "\n",
        "  # loop over all batches of the trainloader\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = CNN_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    batch_loss = loss.item()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    print(f'epoch: {epoch}, batch_{i}, batch_loss: {batch_loss:.3f}')\n",
        "    batch_loss  = 0.0\n",
        "\n",
        "  print(f'epoch loss:{epoch_loss} for epoch_{epoch}')\n",
        "  epoch_loss_mat.append(epoch_loss)\n",
        "  epoch_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "metadata": {
        "id": "kPR8GJaOBdBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_loss_mat)\n",
        "plt.title('CNN model loss of each epoch')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "res63fWL_EpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the model**"
      ],
      "metadata": {
        "id": "6rI91ExkO3xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './CNN_model.pth'\n",
        "torch.save(CNN_model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "ai1vq7AgO7Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, images = CNN_model.predict(images)"
      ],
      "metadata": {
        "id": "u5XP3RsICkWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(labels)):\n",
        "  imshow(images[i])\n",
        "  print(f'label is {labels[i]}')"
      ],
      "metadata": {
        "id": "TsjdOBz-QWPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}